{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes DATA so POWERFUL?\n",
    "It has the power to predict the feature, which is why artificial intelligence is a hot potato"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started with using various APIs from the New York Times, The NEWS, Bloomberg.\n",
    "However, their data could not go past a few months before. Thus, \"The Guardian\" was used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features to notice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_article\n",
    "# Note the parameters used for fetch_article\n",
    "# GET request to retrieve articles \n",
    "# Function efficiently parses JSON response to extract title, content for the articles\n",
    "\n",
    "def fetch_articles(api_key, keyword, page_size):\n",
    "    print(f\"Fetching articles for '{keyword}'...\")\n",
    "    base_url = \"https://content.guardianapis.com/search\"\n",
    "    params = {\n",
    "        'api-key': api_key,\n",
    "        'q': keyword,\n",
    "        'page-size': page_size,\n",
    "        'show-fields': 'body',\n",
    "        'from-date': '2012-01-01',\n",
    "        'to-date': '2012-12-31'\n",
    "    }\n",
    "    constructed_url = requests.Request('GET', base_url, params=params).prepare().url\n",
    "    print(f\"Constructed URL for '{keyword}': {constructed_url}\")\n",
    "   \n",
    "    try:\n",
    "        response = requests.get(constructed_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articles = data['response']['results']\n",
    "        if not articles:\n",
    "            print(f\"No articles found for '{keyword}'.\")\n",
    "            return []\n",
    "        print(f\"Fetched {len(articles)} articles for '{keyword}'.\")\n",
    "        return [(article['webTitle'], article['fields']['body']) for article in articles if 'body' in article['fields']]\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while fetching articles for '{keyword}': {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error while fetching articles for '{keyword}': {err}\")\n",
    "    return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About stop words:\n",
    "Initially, I created stop words by manual, but after a few run through the articles, found out that html factors were included.\n",
    "Also, excluded keywords that intuitively did not seem meaningful (theguardian, the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"https\", \"com\", \"theguardian\", \"href\", \"www\", \"class\", \"block\", \"time\", \"div\", \"id\", \"h2\", \"figure\", \"elements\", \"the\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency Inverse Document Frequency) method was used to extract keywords.\n",
    "TfidfVectorizer: Ignores frequently used English words\n",
    "fit_transform: learns the vocabulary and inverse document frequency weightings, then transforms the documents into a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(documents, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = tfidf_matrix.todense().tolist()\n",
    "    keywords = [sorted(zip(feature_names, doc), key=lambda x: x[1], reverse=True)[:top_n] for doc in dense]\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze_sentiment applies TextBlob (library that provides API for common language processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment polarity: range from [-1,1] for positivty or negativity\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer: Convert text documents into a matrix of token counts (parameters max_df and min_df are used to filter words)\n",
    "# LDA Model: Initialized with the number of topics (n_components) and a random state for reproducibility\n",
    "# Topic Extraction: For each topic, it sorts the words based on their association with the topic and selects the top N words to represent each topic\n",
    "\n",
    "def perform_lda(documents, n_topics=5, n_words=10):\n",
    "    count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vectorizer.fit_transform(documents)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    words = count_vectorizer.get_feature_names_out()\n",
    "    topics = {i: [words[index] for index in topic.argsort()[:-n_words - 1:-1]] for i, topic in enumerate(lda.components_)}\n",
    "    return topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate_ranking: loops through article and calculates average sentiment, returns 20 most used keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rankings(articles, keyword):\n",
    "    print(f\"Aggregating rankings for {len(articles)} articles...\")\n",
    "    word_ranking_sums = Counter()\n",
    "    sentiments = []\n",
    "    for title, body in articles:\n",
    "        word_counts = count_words(body)\n",
    "        word_ranking_sums.update(word_counts)\n",
    "        sentiment = analyze_sentiment(body)\n",
    "        sentiments.append(sentiment)\n",
    "    average_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "    print(f\"Average sentiment for '{keyword}': {average_sentiment}\")\n",
    "    return word_ranking_sums.most_common(20), average_sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_keyword_rankings_interactive: for visualizing\n",
    "used bar graphs to intuitively see which is number 1 to number 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keyword_rankings_interactive(rankings, keyword):\n",
    "    if rankings:\n",
    "        words, scores = zip(*rankings)\n",
    "        fig = px.bar(x=words, y=scores, title=f'Top Word Rankings for {keyword}', labels={'x':'Words', 'y':'Frequency'})\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"No data to plot for '{keyword}'.\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE IS THE INITIAL TRIALS:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 2020 Election:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%203.54.02%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 2016 Election:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%203.54.18%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 2012 Election:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%203.54.22%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE ARE THE FINAL TRIALS:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020 Election:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%205.07.36%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016 Results:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%205.07.41%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2012 Results:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202024-04-11%20at%205.07.55%20PM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the relations between keywords and winner, at first I did not notice any commonality.\n",
    "However, soon I realized that the winner had keywords that had a more \"neutrality\" in the words itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
